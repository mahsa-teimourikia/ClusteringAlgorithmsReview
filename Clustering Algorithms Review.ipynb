{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of the Clustering Algorithms\n",
    "\n",
    "Clustering, in unsupervised learning, deals with the data structure partition without prior knowledge of the characteristics of the partitions. A Clustering algorithm should have the following characteristics:\n",
    "* Instances, in the same cluster, must be similar as much as possible;\n",
    "* Instances, in the different clusters, must be different as much as possible;\n",
    "* Measurement for similarity and dissimilarity must be clear and have the practical meaning;\n",
    "\n",
    "\n",
    "## Similarity and Distance:\n",
    "Distance and similarity are the basis for constructing clustering algorithms. _Distance_ is preferred in case of quantitative data features, while, similarity is mostly used when dealing with the qualitative data features.\n",
    "\n",
    "### Common Distance Functions for Quantitative Features:\n",
    "1) __Minkowski Distance__:\n",
    "Considering two points $X=(x_1,x_2,...,x_d)$ and $Y=(y_1,y_2,...,y_d)$\n",
    "$$D(X,Y)=\\left(\\sum^{d}_{i=1}|x_{i} - x_{i}|^n\\right)^\\frac{1}{n}$$\n",
    "\n",
    "Minkowski distance is typically used with n being 1 or 2, which correspond to the __Manhattan distance__ and the __Euclidean distance__, respectively. In the limiting case of $n$ reaching infinity, we obtain the __Chebyshev distance__:\n",
    "\n",
    "$$\\lim_{n\\rightarrow \\infty }\\left(\\sum^{d}_{i=1}|x_{i} - x_{i}|^n\\right)^\\frac{1}{n} = \\max_{i=1}^{n}|x_i - y_i|$$\n",
    "\n",
    "2) __Standardized Euclidean Distance__\n",
    "\n",
    "$$D(X,Y) = \\left(\\sum^{d}_{i=1}\\left|\\frac{x_{i} - x_{i}}{s_i}\\right|^2\\right)^\\frac{1}{2}$$\n",
    "\n",
    "Where $s$ is the standard deviation. This is equivalent of the weighted euclidean distance based on the deviation.\n",
    "\n",
    "3) __Cosine Distance__\n",
    "$$ 1 - cos \\alpha = 1 - \\frac{X.Y}{||X||||Y||} = 1 - \\frac{\\sum^{n}_{i=1}x_iy_i}{\\sqrt{\\sum^{n}_{i=1}x^2_i}\\sqrt{\\sum^{n}_{i=1}y^2_i}}$$\n",
    "\n",
    "the cosine similarity is most commonly used in high-dimensional positive spaces. For example, in _information retrieval_ and _text mining_, where Cosine similarity gives a  measure of how similar two documents are likely to be in terms of their subject matter. The technique is also used to measure cohesion within clusters in the field of _data mining_.\n",
    "\n",
    "4) __Pearson Correlation Distance__\n",
    "\n",
    "In statistics and in probability theory, distance correlation or distance covariance is a measure of dependence between two paired random vectors of arbitrary, not necessarily equal, dimension. \n",
    "\n",
    " Pearson correlation coefficient is a measure of the linear correlation between two variables X and Y. It has a value between +1 and −1, where 1 is total positive linear correlation, 0 is no linear correlation, and −1 is total negative linear correlation:\n",
    " \n",
    "$$ P_{X,Y} = \\frac{cov(X,Y)}{\\sigma_X\\sigma_Y}$$\n",
    "\n",
    "where:\n",
    "\n",
    "* cov is the covariance\n",
    "* $\\sigma_X$ is the standard deviation of X\n",
    "* $\\sigma_Y$ is the standard deviation of Y \n",
    "\n",
    "5) __Mahalanobis distance__\n",
    "\n",
    "$$\\sqrt{(x_i - y_i)^T S^{-1} (x_i - y_i)}$$\n",
    "\n",
    "This is a measure of the distance between a point P and a distribution D. It is a multi-dimensional generalization of the idea of measuring how many standard deviations away P is from the mean of D. If each of these axes is re-scaled to have unit variance, then the Mahalanobis distance corresponds to standard Euclidean distance in the transformed space.\n",
    "\n",
    "### Common Similarity Functions for Qualitative Features:\n",
    "\n",
    "1) __Jaccard Similarity__\n",
    "\n",
    "Jaccard Similarity  is a statistic used for comparing the similarity and diversity of sample sets. The Jaccard coefficient measures similarity between finite sample sets, and is defined as the size of the intersection divided by the size of the union of the sample sets: \n",
    "\n",
    "$$J(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "2) __Hamming Similarity__\n",
    "\n",
    "In information theory, the Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. In other words, it measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other.\n",
    "\n",
    "## Evaluation Indicators\n",
    "The purpose of using the evaluation indicators is to test the validity of the algorithm.\n",
    "\n",
    "### Internal Evaluation Indicators\n",
    "When a clustering result is evaluated based on the data that was clustered itself, this is called internal evaluation. These methods usually assign the best score to the algorithm that produces clusters with high similarity within a cluster and low similarity between clusters. \n",
    "\n",
    "One drawback of using internal criteria in cluster evaluation is that high scores on an internal measure do not necessarily result in effective information retrieval applications.\n",
    "\n",
    "1) __Davies-Bouldin Indicator__\n",
    "\n",
    "$$DB = \\frac{1}{n}\\sum_{i=1}^{n} \\max_{j\\neq i}\\left( \\frac{\\sigma_i + \\sigma_j}{d(c_i,c_j)}\\right) $$\n",
    "\n",
    "where $n$ is the number of clusters, $c_x$ is the centroid of cluster $x$. $\\sigma_{x}$ is the average distance of all elements in cluster $x$ to centroid $c_{x}$, and $d(c_{i},c_{j})$ is the distance between centroids $c_{i}$ and $c_{j}$. \n",
    "\n",
    "Since algorithms that produce clusters with low intra-cluster distances (high intra-cluster similarity) and high inter-cluster distances (low inter-cluster similarity) will have a low Davies–Bouldin index, the clustering algorithm that produces a collection of clusters with the smallest Davies–Bouldin index is considered the best algorithm based on this criterion.\n",
    "\n",
    "2) __Dunn Index__\n",
    "\n",
    "The Dunn index aims to identify dense and well-separated clusters. It is defined as the ratio between the minimal inter-cluster distance to maximal intra-cluster distance. For each cluster partition, the Dunn index can be calculated by the following formula:\n",
    "\n",
    "$$ D = \\frac{\\min_{1\\leq i \\leq j \\leq n} d(i,j)}{\\max_{1 \\leq k \\leq n} {d}'(k)}$$\n",
    "\n",
    "where $d(i,j)$ represents the distance between clusters $i$ and $j$, and ${d}'(k)$ measures the intra-cluster distance of cluster $k$.\n",
    "\n",
    "3) __Silhouette coefficient__\n",
    "\n",
    "The silhouette coefficient contrasts the average distance to elements in the same cluster with the average distance to elements in other clusters. Objects with a high silhouette value are considered well clustered, objects with a low value may be outliers. This index works well with k-means clustering, and is also used to determine the optimal number of clusters.\n",
    "\n",
    "### External Evaluation Indicators\n",
    "\n",
    "In external evaluation, clustering results are evaluated based on data that was not used for clustering, such as known class labels and external benchmarks. Such benchmarks consist of a set of pre-classified items, and these sets are often created by (expert) humans. Thus, the benchmark sets can be thought of as a gold standard for evaluation.\n",
    "\n",
    "These types of evaluation methods measure how close the clustering is to the predetermined benchmark classes. However, it has recently been discussed whether this is adequate for real data, or only on synthetic data sets with a factual ground truth, since classes can contain internal structure, the attributes present may not allow separation of clusters or the classes may contain anomalies.\n",
    "\n",
    "1) __Purity__\n",
    "\n",
    "Purity is a measure of the extent to which clusters contain a single class.\n",
    "\n",
    "$$ \\frac{1}{N}\\sum_{m \\in M} \\max_{d \\in D}|m \\cap d|$$\n",
    "\n",
    "Which is for each cluster, count the number of data points from the most common class in said cluster. Now take the sum over all clusters and divide by the total number of data points.\n",
    "\n",
    "* $M$ set of clusters\n",
    "* $D$ set of classes\n",
    "* $N$ number of data points\n",
    "\n",
    "2) __Rand measure__\n",
    "\n",
    "The Rand index computes how similar the clusters (returned by the clustering algorithm) are to the benchmark classifications. One can also view the Rand index as a measure of the percentage of correct decisions made by the algorithm.\n",
    "\n",
    "$$RI = \\frac{TP + TN}{TP+TN+FP+FN}$$\n",
    "\n",
    "One issue with the Rand index is that false positives and false negatives are equally weighted. This may be an undesirable characteristic for some clustering applications. The F-measure addresses this concern, as does the chance-corrected adjusted Rand index.\n",
    "\n",
    "3) __F-measure__\n",
    "\n",
    "Considering the definition of the precision $P$ and recall $R$ as follows:\n",
    "$$ P = \\frac{TP}{TP + FP}$$\n",
    "$$ R = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "The F-measure can be used to balance the contribution of false negatives by weighting recall through a parameter $\\beta \\geq 0$:\n",
    "\n",
    "$$ F_{\\beta} = \\frac{(\\beta^2 + 1).P.R}{\\beta^2.P + R}$$\n",
    "\n",
    "Notice that when $\\beta =0$, $F_{0}=P$. In other words, recall has no impact on the F-measure when $\\beta =0$, and increasing $\\beta$ allocates an increasing amount of weight to recall in the final F-measure.\n",
    "\n",
    "Also note that TN is not taken into account and can vary from 0 upward without bound.\n",
    "\n",
    "4) __Jaccard index__\n",
    "The Jaccard index is used to quantify the similarity between two datasets. The Jaccard index takes on a value between 0 and 1. An index of 1 means that the two dataset are identical, and an index of 0 indicates that the datasets have no common elements.\n",
    "\n",
    "$$J(A,B) = \\frac{|A \\cap B|}{|A \\cup B|} = \\frac{TP}{TP+FP+FN}$$\n",
    "\n",
    "This is simply the number of unique elements common to both sets divided by the total number of unique elements in both sets.\n",
    "\n",
    "Also note that TN is not taken into account and can vary from 0 upward without bound.\n",
    "\n",
    "5) __Dice Index__\n",
    "\n",
    "The Dice symmetric measure doubles the weight on TP while still ignoring TN and is equivalent to F1 – the F-measure with $\\beta = 1$ \n",
    "\n",
    "$$RDSC = \\frac{2TP}{2TP+FP+FN}$$\n",
    "\n",
    "6) __Fowlkes-Mallows Index__\n",
    "It computes the similarity between the clusters returned by the clustering algorithm and the benchmark classifications.\n",
    "\n",
    "$$FM = \\sqrt{\\frac{TP}{TP+FP}.\\frac{TP}{TP+FN}}$$\n",
    "\n",
    "FM index is also referred to as the G-measure. Higher values of the FM index indicates the higher similarity of the benchmark classifications and the clusters resulted from the clustering algorithm.\n",
    "\n",
    "7) __Mutual Information__ \n",
    "This is a measure based on information theory which can detecct the non-linear similarity between two clusterings by measuring how much information is shared between a clustering and a ground-truth classification.\n",
    "\n",
    "8) __Confusion Matrix__\n",
    "This is used to visualize the results of a classification/clustering algorithm. It shows how different a cluster is from the gold standard cluster.\n",
    " \n",
    "### Cluster Tendency\n",
    "Cluster tendency is to measure to what degree clusters exist in the data to be clustered, and may be performed as an initial test, before attempting clustering.\n",
    "\n",
    "One way to do this is to compare the data against random data that should not have clusters.\n",
    "\n",
    "1) __Hopkins Statistics__\n",
    "\n",
    "Let $X$ be the set of $m$ sample (without replacement) of $m \\ll n$ data points with members $x_{i}$. Also generate a set $Y$ of $m$ uniformly randomly distributed data points. Now define two distance measures,  $u_{i}$ to be the distance of $y_{i}\\in Y$ from its nearest neighbor in $X$ and $w_{i}$ to be the distance of $x_{i}\\in X$ from its nearest neighbor in $X$. We then define the Hopkins statistic as:\n",
    "\n",
    "$$ H=\\frac{\\sum^{m}_{i=1} u^d_i}{\\sum^{m}_{i=1} u^d_i + \\sum^{m}_{i=1} w^d_i}$$\n",
    "\n",
    "Using this definition, clustere data should tend to have values nearer to $1$, while random uniform data should tend to have values near to $0.5$\n",
    "\n",
    "Note that data containing just a single _Gaussian_ will also score close to 1, as this statistic measures deviation from a uniform distribution, not multimodality, making this statistic largely useless in application (as real data never is remotely uniform).\n",
    "\n",
    "## Clustering Algorithms\n",
    "\n",
    "### Clustering Algorithm Types\n",
    "\n",
    "Broadly speaking, clustering can be divided into two subgroups :\n",
    "\n",
    "* __Hard Clustering:__ In hard clustering, each data point either belongs to a cluster completely or not. For example, in the above example each customer is put into one group out of the 10 groups.\n",
    "* __Soft Clustering:__ In soft clustering, instead of putting each data point into a separate cluster, a probability or likelihood of that data point to be in those clusters is assigned. For example, from the above scenario each costumer is assigned a probability to be in either of 10 clusters of the retail store.\n",
    "\n",
    "\n",
    "Clustering algorithms can be categorized based on their cluster model.\n",
    "\n",
    "### Connectivity-Based Custering (Hierarchical Clustering)\n",
    "\n",
    "This is based on the core idea of objects being more related to nearby objects than to objects farther away. And hence, in these algorithms the data points form clusters based on their distance.\n",
    "\n",
    "In this definition, clusters are defined based on a maximum distance which includes the data points of the cluster. In this way, different clusters will be formed at different distances. These models do not provide a single partitioning of the data set. Insead, they result in an extensive hierarchy of clusters that merge with each other at certain distances.\n",
    "\n",
    "Usually dendrograms, which are tree diagrams to show the arrangement of the clusters, are used for representing the hierarchical clustering models.\n",
    "\n",
    "#### The Difference Between Hierarchical Clustering Methods\n",
    "\n",
    "* The distance function\n",
    "* The Linkage Criterion:\n",
    "     * single-linkage clustering: the minimum of object distances, \n",
    "     * complete linkage clustering: the maximum of object distances,\n",
    "     * UPGMA or WPGMA: \"Unweighted or Weighted Pair Group Method with Arithmetic Mean\", also known as average linkage clustering. \n",
    "* Furthermore, hierarchical clustering can be:\n",
    "     * Agglomerative: starting with single elements and aggregating them into clusters\n",
    "     * Divisive: starting with the complete data set and dividing it into partitions. \n",
    "     \n",
    "#### Agglomerative and Divisive Hierarchical Clustering Methods\n",
    "\n",
    "Hierarchical clustering algorithms actually fall into 2 categories: top-down or bottom-up. Bottom-up algorithms treat each data point as a single cluster at the outset and then successively merge (or agglomerate) pairs of clusters until all clusters have been merged into a single cluster that contains all data points. Bottom-up hierarchical clustering is therefore called hierarchical agglomerative clustering or HAC. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. Check out the graphic below for an illustration before moving on to the algorithm steps:\n",
    "\n",
    "<img src=\"images/agglomerative.gif\">\n",
    "     \n",
    "#### Pros and Cons of the Hierarchical Clustering Methods\n",
    "\n",
    "* These methods will not produce a unique partitioning of the data set, but a hierarchy from which the user still needs to choose appropriate clusters.\n",
    "* They are not very robust towards outliers, which will either show up as additional clusters or even cause other clusters to merge, known as \"chaining phenomenon\", in particular with single-linkage clustering.\n",
    "* In the general case, the complexity is $O ( n^3 )$  for agglomerative clustering and $O ( 2^{n − 1})$ for divisive clustering, which makes them too slow for large data sets. For some special cases, optimal efficient methods (of complexity $O ( n^2)$) are known: SLINK for single-linkage and CLINK for complete-linkage clustering. \n",
    "\n",
    "### Centroid-Based Clustering\n",
    "\n",
    "* In centroid-based clustering, clusters are represented by a central vector, which may not necessarily be a member of the data set. \n",
    "* Example: K-Means Clustering\n",
    "* K-Means only find a local optimum, and is commonly run multiple times with different random initializations. \n",
    "* Variations of k-means often include optimizations such as choosing the best of multiple runs, but also restricting the centroids to members of the data set (k-medoids), choosing medians (k-medians clustering), choosing the initial centers less randomly (k-means++) or allowing a fuzzy cluster assignment (fuzzy c-means). \n",
    "\n",
    "#### Example of the K-Means Clustering Algorithm\n",
    "<img src=\"images/kmeans.gif\" />\n",
    "\n",
    "#### Pros and Cons of the Centroid-Based Clustering\n",
    "\n",
    "* Most k-means-type algorithms require the number of clusters – k – to be specified in advance, which is considered to be one of the biggest drawbacks of these algorithms. \n",
    "* The algorithms prefer clusters of approximately similar size, as they will always assign an object to the nearest centroid. This often leads to incorrectly cut borders of clusters (which is not surprising since the algorithm optimizes cluster centers, not cluster borders). \n",
    "\n",
    "### Distribution-based clustering\n",
    "\n",
    "This is the clustering model that is most closely related to statistics is based on distribution models. Here clusters can then easily be defined as objects belonging most likely to the same distribution. \n",
    "\n",
    "#### Gaussian Mixture Models (GMM)\n",
    "\n",
    "One of the major drawbacks of K-Means is its naive use of the mean value for the cluster center. One example in which K-Means fails to cluster the data correctly can be seen below:\n",
    "\n",
    "<img src=\"images/kmeans_fail.png\"/>\n",
    "\n",
    "Gaussian Mixture Models (GMMs) give us more flexibility than K-Means. With GMMs we assume that the data points are Gaussian distributed; this is a less restrictive assumption than saying they are circular by using the mean. That way, we have two parameters to describe the shape of the clusters: the mean and the standard deviation! \n",
    "\n",
    "In order to find the parameters of the Gaussian for each cluster (e.g the mean and standard deviation) we will use an optimization algorithm called Expectation–Maximization (EM). Take a look at the graphic below as an illustration of the Gaussians being fitted to the clusters. \n",
    "\n",
    "<img src=\"images/GMM.gif\"/>\n",
    "\n",
    "GMM using the expectation-maximization algorithm can be described as follows:\n",
    "* Here, the data set is usually modeled with a fixed (to avoid overfitting) number of Gaussian distributions that are initialized randomly and whose parameters are iteratively optimized to better fit the data set. \n",
    "* This will converge to a local optimum, so multiple runs may produce different results. \n",
    "* In order to obtain a hard clustering, objects are often then assigned to the Gaussian distribution they most likely belong to; for soft clusterings, this is not necessary.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Prons and Cons of the Distribution-Based Clustering\n",
    "\n",
    "* They suffer from one key problem known as overfitting, unless constraints are put on the model complexity.\n",
    "* A more complex model will usually be able to explain the data better, which makes choosing the appropriate model complexity inherently difficult.\n",
    "* It produces complex models for clusters that can capture correlation and dependence between attributes. However, these algorithms put an extra burden on the user: for many real data sets, there may be no concisely defined mathematical model (e.g. assuming Gaussian distributions is a rather strong assumption on the data).\n",
    "\n",
    "### Density-based clustering\n",
    "\n",
    "In density-based clustering methods, clusters are defined as areas of higher density than the remainder of the data set. \n",
    "Objects in sparse areas - that are required to separate clusters - are usually considered to be noise and border points. \n",
    "The most popular density based clustering method is DBSCAN.\n",
    "\n",
    "#### DBSCAN\n",
    "\n",
    "In contrast to many newer methods, it features a well-defined cluster model called \"density-reachability\".\n",
    "Similar to linkage based clustering, it is based on connecting points within certain distance thresholds. However, it only connects points that satisfy a density criterion, in the original variant defined as a minimum number of other objects within this radius.\n",
    "\n",
    "##### Example of the DBSCAN Clustering Algorithm\n",
    "\n",
    "<img src=\"images/DBSCAN.gif\"/>\n",
    "\n",
    "#### Other Density-Based Clustering Methods\n",
    "\n",
    "* OPTICS is a generalization of DBSCAN that removes the need to choose an appropriate value for the range parameter $\\varepsilon$ , and produces a hierarchical result related to that of linkage clustering. \n",
    "* DeLi-Clu, Density-Link-Clustering combines ideas from single-linkage clustering and OPTICS, eliminating the ε parameter entirely and offering performance improvements over OPTICS by using an R-tree index.\n",
    "\n",
    "* Mean-shift is a clustering approach where each object is moved to the densest area in its vicinity, based on kernel density estimation. Eventually, objects converge to local maxima of density. Similar to k-means clustering, these \"density attractors\" can serve as representatives for the data set, but mean-shift can detect arbitrary-shaped clusters similar to DBSCAN. Due to the expensive iterative procedure and density estimation, mean-shift is usually slower than DBSCAN or k-Means. Besides that, the applicability of the mean-shift algorithm to multidimensional data is hindered by the unsmooth behaviour of the kernel density estimate, which results in over-fragmentation of cluster tails.\n",
    "\n",
    "##### Examples of Mean-Shift Clustering Algorithm\n",
    "\n",
    "Mean shift clustering is a sliding-window-based algorithm that attempts to find dense areas of data points. It is a centroid-based algorithm meaning that the goal is to locate the center points of each group/class, which works by updating candidates for center points to be the mean of the points within the sliding-window. These candidate windows are then filtered in a post-processing stage to eliminate near-duplicates, forming the final set of center points and their corresponding groups. Check out the graphic below for an illustration:\n",
    "<img src=\"images/meanshift.gif/\">\n",
    "\n",
    "An illustration of the entire process from end-to-end with all of the sliding windows is show below. Each black dot represents the centroid of a sliding window and each gray dot is a data point.\n",
    "<img src=\"images/meanshift-all.gif/\">\n",
    "\n",
    "\n",
    "#### Pros and Cons of the DBSCAN\n",
    "\n",
    "* Its complexity is fairly low.\n",
    "* There is no need to run it multiple times\n",
    "* The key drawback of DBSCAN and OPTICS is that they expect some kind of density drop to detect cluster borders. \n",
    "    * On data sets with, overlapping Gaussian distributions the cluster borders produced by these algorithms will often look arbitrary, because the cluster density decreases continuously. \n",
    "    * On a data set consisting of mixtures of Gaussians, these algorithms are nearly always outperformed by methods such as EM clustering that are able to precisely model this kind of data. \n",
    "    \n",
    "\n",
    "\n",
    "### Recent developments\n",
    "\n",
    "* CLARANS and BIRCH are developed for improving the performance of existing algorithms.\n",
    "\n",
    "* With the recent need to process larger and larger data sets (also known as big data), the willingness to trade semantic meaning of the generated clusters for performance has been increasing. This led to the development of pre-clustering methods such as canopy clustering, which can process huge data sets efficiently, but the resulting \"clusters\" are merely a rough pre-partitioning of the data set to then analyze the partitions with existing slower methods such as k-means clustering. \n",
    "\n",
    "* For high-dimensional data, many of the existing methods fail due to the curse of dimensionality, which renders particular distance functions problematic in high-dimensional spaces. This led to new clustering algorithms for high-dimensional data that focus on subspace clustering (where only some attributes are used, and cluster models include the relevant attributes for the cluster) and correlation clustering that also looks for arbitrary rotated (\"correlated\") subspace clusters that can be modeled by giving a correlation of their attributes. Examples for such clustering algorithms are CLIQUE and SUBCLU.\n",
    "\n",
    "* Ideas from density-based clustering methods (in particular the DBSCAN/OPTICS family of algorithms) have been adopted to subspace clustering (HiSC, hierarchical subspace clustering and DiSH) and correlation clustering (HiCO, hierarchical correlation clustering, 4C using \"correlation connectivity\" and ERiC exploring hierarchical density-based correlation clusters). \n",
    "\n",
    "* Several different clustering systems based on mutual information have been proposed. One is Marina Meilă's variation of information metric; another provides hierarchical clustering.Using genetic algorithms, a wide range of different fit-functions can be optimized, including mutual information. Also message passing algorithms, a recent development in computer science and statistical physics, has led to the creation of new types of clustering algorithms.\n",
    "\n",
    "### How Most Prominent Clustering Algorithms Perform:\n",
    "<img src=\"images/clustering.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "[1] Xu, D. & Tian, Y., A Comprehensive Survey of Clustering Algorithms, Annals of Data Science (2015) 2: 165, Springer. https://doi.org/10.1007/s40745-015-0040-1\n",
    "[2] https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/\n",
    "[3] https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
